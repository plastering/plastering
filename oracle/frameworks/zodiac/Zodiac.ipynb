{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zodiac: Source Code and Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Date: May 30, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modern buildings consist of many different types of infrastructure such as lighting, air conditioning, power, water. For operation and maintenance of these systems with minimal manual supervision, networked sensors are deployed across the building so that it can be monitored remotely. <a href=\"https://en.wikipedia.org/wiki/Building_management_system\"> Building Management Systems (BMS)</a> are software systems that collect data from installed sensors, allow remote control of equipment and provide visualizations for the maintenance personnel. \n",
    "\n",
    "Modern BMSes have thousands of data points per building, and these data points correspond to installed sensors, actuators of equipment as well as configuration parameters. With modern data processing and control mechanisms it is possible to exploit these data points to create useful and innovative building applications such as personalized control, fault detection, demand response management, model predictive control, power grid stability and many more. However, a major impediment to deployment of these applications is that the data points are organized for building domain experts and not computer algorithms. As a result, the metadata that describes the context of the data points in the BMS has errors, extraneous notes, vendor specific notations and other inconsistencies which make it difficult for a machine to interpret the data. Our project Zodiac exploits machine learning techniques to map the raw building metadata to a consistent format so that applications can be developed on top of a common interface and reused across multiple buildings. \n",
    "\n",
    "Our full research paper which describes the building metadata problem and the Zodiac algorithm can be found <a href=\"http://dl.acm.org/citation.cfm?id=2821674\">here</a>. The Zodiac project home page where we share our raw building metadata, manually labelled ground truth data point types and this source code page can be found <a href=\"http://www.synergylabs.org/bharath/zodiac.html\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are making the source code available using the Jupyter notebook with explanations alongside so it is easy to follow and replicate our results. \n",
    "\n",
    "#### Input File Paths\n",
    "The notebook code assumes that the raw metadata and manually labelled ground truth files are available in \"metadata\" directory at the same level as the notebook.\n",
    "\n",
    "#### Python Libraries\n",
    "We heavily use the <a href=\"http://scikit-learn.org/stable/\">Python Scikit Learn</a> library, which contains implementations of popular machine learning algorithms such as hierarchical clustering and random forest classifier used in our algorithm. We use Python <a href=\"http://www.numpy.org/\">numpy</a> and <a href=\"http://pandas.pydata.org/\">pandas</a> libraries to store and manipulate large records of metadata. These data structures work well with Scikit Learn modules.\n",
    "\n",
    "We use Python <a href=\"https://docs.python.org/2/library/re.html\">re</a> for regular expressions and <a href=\"http://matplotlib.org/\">matplotlib</a> for plotting. We also use <a href=\"https://docs.python.org/2/library/shelve.html\">shelve</a> and <a href=\"https://docs.python.org/2/library/pickle.html\">pickle</a> for object serialization and non-volatile storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.cluster.vq import *\n",
    "import operator\n",
    "from matplotlib import pyplot as plt   \n",
    "import pickle as pkl\n",
    "import shelve\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The file \"bacnet_devices.shelve\" contains the building points metadata for about 55 buildings at <a href=\"http://ucsd.edu/\">University of California, San Diego</a> (UCSD). We obtain this metadata using <a href=\"http://www.bacnet.org/\">BACnet</a>, a standard building automation network communication protocol. \n",
    "\n",
    "When loaded, the shelve file becomes a Python dictionary. The format of the metadata is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### This is not part of the source code ####\n",
    "[\"bacnet_device_id\": {\n",
    "           \"props\":{ \"device_id\": 557 },\n",
    "           \"objs\":{\n",
    "               \"sensor_1\":{\n",
    "                   \"props\":{\n",
    "                       \"type\": 0,\n",
    "                       \"type_str\": \"analog input\", #BACnet encodes type as enumerator. 0 stands for analog input\n",
    "                       \"instance\": 230523\n",
    "                   },\n",
    "                   \"name\":\"NAE 57 N2 1 VAV 5 ZN T\",\n",
    "                   \"source_id\":\"557_0_3001187\", #Concatenation of device id, type and object instance\n",
    "                   \"description\":\"Zone Temperature\", #Human readable point description\n",
    "                   \"unit\": 64, #BACnet encoded units. 64 stands for Fahrenheit\n",
    "                   \"jci_name\": \"BH.1STFLR.RM-1511.VAV-5.ZN-T\" \n",
    "               }\n",
    "           }\n",
    " },\n",
    " \"bacnet_device_id_2\": {\n",
    "   ...\n",
    " },\n",
    " ...\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"bacnet_device_id\" corresponds to a middlebox that hosts up to 4000 building data points. One building can be assigned several middleboxes based on its requiments and the network design. All of the 55 buildings we obtain our metadata from are managed by the vendor <a href=\"http://www.johnsoncontrols.com/\">Johnson Control Inc.</a>, and some of the metadata organizing may be specific to this vendor.\n",
    "\n",
    "Each point is identified by its \"instance\" within the middlebox, and we define a \"source_id\" as a university wide unique identifier for the point. There are two names associated with the point. The \"name\" corresponds to BACnet name property, and the name is based on network architecture. The \"jci_name\" is a proprietary BACnet field used by Johnson Controls, and the name is assigned relative to building location hierarchy. The last part of the name (\"ZN T\" above) encodes an abbreviation of the data point type. The human readable data point type is given in \"description\" (e.g. \"Zone Temperature\"). \n",
    "\n",
    "BACnet also encodes the data type and input/output in one field referred to as \"type\" above. For example, \"analog input\" means the data type is float and the point is of input type (e.g. sensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import shelve dictionary containing all bacnet devices\n",
    "sensors_dict = shelve.open('metadata/bacnet_devices.shelve','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#device_list filters the NAE for a particular building. This is currently manual. It can be automated \n",
    "#if building names are known.\n",
    "\n",
    "#bonner hall\n",
    "device_list = [\n",
    "                \"557\",\n",
    "                \"607\",\n",
    "                \"608\",\n",
    "                \"609\",\n",
    "                \"610\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Parse the data in the dictionary as filtered by device_list\n",
    "#Gives us a sensor_list with sensor information of a building\n",
    "sensor_list = []\n",
    "names_list = []\n",
    "names_listWithDigits = [] \n",
    "sensor_type_namez=[]\n",
    "desc_list = []\n",
    "unit_list = []\n",
    "type_str_list = []\n",
    "type_list = []\n",
    "jci_names_list = []\n",
    "source_id_set = set([])\n",
    "for nae in device_list:\n",
    "    device = sensors_dict[nae]\n",
    "    h_dev = device['props']\n",
    "    for sensor in device['objs']:\n",
    "        h_obj = sensor['props']\n",
    "        source_id = str(h_dev['device_id']) + '_' + str(h_obj['type']) + '_' + str(h_obj['instance'])\n",
    "        \n",
    "        if h_obj['type'] not in (0,1,2,3,4,5,13,14,19):\n",
    "            continue\n",
    "        \n",
    "        if source_id in source_id_set:\n",
    "            continue\n",
    "        else:\n",
    "            source_id_set.add(source_id)\n",
    "        \n",
    "        #create individual lists\n",
    "        #remove numbers from names because they do not indicate type of sensor\n",
    "        names_listWithDigits.append(sensor['jci_name']) \n",
    "        sensor_type_namez.append(sensor['sensor_type'])\n",
    "        names_list.append(''.join([c for c in sensor['name'] if not c.isdigit()]))\n",
    "        desc_list.append(''.join([c for c in sensor['desc'] if not c.isdigit()]))\n",
    "        jci_names_list.append(''.join([c for c in sensor['jci_name'] if not c.isdigit()]))\n",
    "        #convert string to dictionary for categorical vectorization\n",
    "        unit_list.append({str(sensor['unit']):1})\n",
    "        type_str_list.append({str(h_obj['type_str']):1})\n",
    "        type_list.append({str(h_obj['type']):1})\n",
    "        \n",
    "        #create a flat list of dictionary to avoid using json file\n",
    "        sensor_list.append({'source_id': source_id, \n",
    "                            'name': sensor['name'], \n",
    "                            'description': sensor['desc'],\n",
    "                            'unit': sensor['unit'],\n",
    "                            'type_string': h_obj['type_str'],\n",
    "                            'type': h_obj['type'],\n",
    "                            #'device_id': h_obj['device_id'],\n",
    "                            'jci_name': sensor['jci_name'],\n",
    "                            #add data related characteristics here\n",
    "                        })\n",
    "sensor_df = pd.DataFrame(sensor_list)\n",
    "sensor_df = sensor_df.set_index('source_id')\n",
    "sensor_df = sensor_df.groupby(sensor_df.index).first()\n",
    "print len(sensor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a bag of words from sensor string metadata. Vectorize so that it can be used in ML algorithms.\n",
    "namevect = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "namebow = scipy.sparse.coo_matrix(namevect.fit_transform(names_list))\n",
    "\n",
    "descvect = CountVectorizer() \n",
    "descbow = scipy.sparse.coo_matrix(descvect.fit_transform(desc_list))\n",
    "\n",
    "unitvect = DictVectorizer() \n",
    "unitbow = scipy.sparse.coo_matrix(unitvect.fit_transform(unit_list))\n",
    "\n",
    "type_str_vect = DictVectorizer() \n",
    "type_str_bow = scipy.sparse.coo_matrix(type_str_vect.fit_transform(type_str_list))\n",
    "\n",
    "typevect = DictVectorizer() \n",
    "typebow = scipy.sparse.coo_matrix(typevect.fit_transform(type_list))\n",
    "\n",
    "jcivect = CountVectorizer() \n",
    "jcibow = scipy.sparse.coo_matrix(jcivect.fit_transform(jci_names_list))\n",
    "\n",
    "feature_set = jcivect.get_feature_names()+ \\\n",
    "              descvect.get_feature_names()+ \\\n",
    "              unitvect.get_feature_names()+ \\\n",
    "              type_str_vect.get_feature_names()+ \\\n",
    "              typevect.get_feature_names()\n",
    "              \n",
    "\n",
    "final_bow = scipy.sparse.hstack([\n",
    "                                 #namebow,\n",
    "                                 descbow,\n",
    "                                 unitbow,\n",
    "                                 type_str_bow,\n",
    "                                 typebow,\n",
    "                                 jcibow\n",
    "                                ]) \n",
    "bow_array = final_bow.toarray() # this is the bow for each sensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hierarchical agglomerative clustering \n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import scipy.cluster.hierarchy as hier\n",
    "\n",
    "num_of_sensors = len(bow_array)\n",
    "a = np.array(bow_array[:num_of_sensors])\n",
    "z = linkage(a,metric='cityblock',method='complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Apply threshold to hierarchical tree to obtain individual clusters. Results stored in equip_map\n",
    "dists = list(set(z[:,2]))\n",
    "thresh = (dists[2] + dists[3]) /2 \n",
    "print \"Threshold: \", thresh\n",
    "b = hier.fcluster(z,thresh, criterion='distance')\n",
    "cluster_map = {}\n",
    "equip_map = {}\n",
    "for i in range(len(b)):\n",
    "    cluster_map[names_list[i]] = b[i]\n",
    "    print i, names_list[i], b[i]\n",
    "    if b[i] in equip_map:\n",
    "        equip_map[b[i]][\"sensors\"].append(sensor_list[i])\n",
    "        equip_map[b[i]][\"sensor_ids\"].append(i)\n",
    "    else:\n",
    "        equip_map[b[i]] = {\"sensors\":[sensor_list[i]]}\n",
    "        equip_map[b[i]][\"sensor_ids\"] = [i]\n",
    "    sensor_list[i]['equip_cluster_id'] = b[i]\n",
    "sorted_map = sorted(cluster_map.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read ground truth sensor types\n",
    "import csv\n",
    "building = 'bonner'\n",
    "ground_truth_list = []\n",
    "with open('metadata/'+building+'_sensor_types.csv') as ground_truth_file:\n",
    "    csv_reader = csv.DictReader(ground_truth_file)\n",
    "    for row in csv_reader:\n",
    "        ground_truth_list.append(row)\n",
    "sensor_type_map = {s['source_id']:s['sensor_type'] for s in ground_truth_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merges the clusters formed by hierarchical clustering based on \"description\" tag. \n",
    "equip_desc_map = {}\n",
    "sensor_abbrvs = [s['jci_name'].split('.')[-1].lower() if '.' in s['jci_name'] else s['jci_name'] for s in ground_truth_list]\n",
    "#sensor_abbrvs = [re.sub('[^a-z ]', '', s) for s in sensor_abbrvs]\n",
    "\n",
    "for k,v in equip_map.iteritems():\n",
    "    #print v\n",
    "    desc_list = [s['description'].lower() for s in v['sensors']]\n",
    "    desc_list = [re.sub('[^a-z ]', '', d) for d in desc_list]\n",
    "    desc_list = [sensor_abbrvs[i] if d == '' else d for i,d in enumerate(desc_list)]\n",
    "    if len(set(desc_list)) == 1:\n",
    "        if desc_list[0] in equip_desc_map and desc_list[0] != '':\n",
    "            equip_desc_map[desc_list[0]]['sensors'] += v['sensors']\n",
    "            equip_desc_map[desc_list[0]]['sensor_ids'] += v['sensor_ids']\n",
    "        elif desc_list[0] == '':\n",
    "            equip_desc_map[k] = v\n",
    "        else:\n",
    "            equip_desc_map[desc_list[0]] = v\n",
    "    else:\n",
    "        equip_desc_map[k] = v\n",
    "    \n",
    "#print \"merged cluster:\", len(equip_desc_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get ground truth set\n",
    "#equip_map = equip_desc_map #Uncomment for using merged clusters\n",
    "# Manually label say 10 clusters and hence multiple sensors. \n",
    "import random\n",
    "num_manual_labels = 10\n",
    "sensor_labels = []\n",
    "sensor_bow = []\n",
    "labeled_equip_keys = []\n",
    "equip_cluster_lens = {k:len(v['sensors']) for k,v in equip_map.iteritems()}\n",
    "sorted_equip_keys = sorted(equip_cluster_lens.items(), key=operator.itemgetter(1), reverse=True)\n",
    "for i in range(num_manual_labels):\n",
    "#for c_id in equip_map.keys()[:num_manual_labels]:\n",
    "    c_id = random.choice(equip_map.keys())\n",
    "    #c_id = sorted_equip_keys[i][0]\n",
    "    labeled_equip_keys.append(c_id)\n",
    "    for ix,i in enumerate(equip_map[c_id]['sensor_ids']):\n",
    "            sensor_bow.append(bow_array[i])\n",
    "            source_id = sensor_list[i]['source_id']\n",
    "            sensor_labels.append(sensor_type_map[source_id])\n",
    "sensor_bow = np.array(sensor_bow)\n",
    "sensor_labels = np.array(sensor_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#learn a model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.mixture import GMM\n",
    "from sklearn.mixture import DPGMM\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "le = LabelEncoder()\n",
    "le.fit(sensor_labels)\n",
    "#print list(le.classes_)\n",
    "train_labels = le.transform(sensor_labels)\n",
    "model = RandomForestClassifier(n_estimators=400, random_state=0)\n",
    "model.fit(sensor_bow,sensor_labels)\n",
    "#model.fit(sensor_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_model_on_all_clusters( ): # model, T_low, T_high, ....\n",
    "    # This method uses global variables including model, T_low, T_high and several others. \n",
    "    # Goal: apply model on all clusters and determine correctness of \"confident predictions\" \n",
    "    # and manually label \"very low confidence\" ones \n",
    "\n",
    "    global sensor_labels\n",
    "    global sensor_bow \n",
    "    global labeled_equip_keys \n",
    "    \n",
    "    # Iteratively apply Random Forest to label new sensors \n",
    "    change_thresholds = True \n",
    "    n_wrong_confident_sensor_pred = 0\n",
    "    sensor_bow = list(sensor_bow)\n",
    "    sensor_labels = list(sensor_labels) \n",
    "    n_high_confidence_sensors = 0\n",
    "    n_manually_labeled_thisepoch = 0 # epoch = whatever happens after (re-) training RF models\n",
    "\n",
    "    for p in equip_map.keys(): # for each cluster \n",
    "    #for p in sorted_equip_keys:\n",
    "        #p = p[0]\n",
    "\n",
    "        # Escape if already labeled. \n",
    "        if p in labeled_equip_keys:\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Get sensors from this cluster. \n",
    "        sample_bow = []\n",
    "        for k in equip_map[p]['sensor_ids']:\n",
    "            sample_bow.append(bow_array[k])\n",
    "        sample_bow = np.array(sample_bow)\n",
    "\n",
    "\n",
    "        # Apply trained model: \n",
    "        confidence = model.predict_proba(sample_bow)\n",
    "        prediction_label = model.predict(sample_bow)\n",
    "        # Get overall max confidence for any sensor in cluster: \n",
    "        max_c = 0\n",
    "        for c in confidence:\n",
    "            max_c = max(np.append(c,[max_c]))\n",
    "\n",
    "\n",
    "        # Compare with Thresholds. \n",
    "        flag = 0    \n",
    "        if max_c < T_low:\n",
    "            flag = 1\n",
    "        if max_c > T_high:\n",
    "            flag = 2        \n",
    "\n",
    "        if flag==1: \n",
    "            n_manually_labeled_thisepoch+=1 \n",
    "\n",
    "        \n",
    "        # Handle the cluster beyond threshold: \n",
    "        if flag>0: \n",
    "            change_thresholds = False \n",
    "            labeled_equip_keys.append(p)  \n",
    "\n",
    "            # For each sensor in this cluster: \n",
    "            for k in range(len(equip_map[p]['sensors'])):  \n",
    "                sourceid = equip_map[p]['sensors'][k]['source_id']\n",
    "                true_type = sensor_type_map[sourceid] \n",
    "                pred_type = prediction_label[k]              \n",
    "\n",
    "                if flag==2: \n",
    "                    n_high_confidence_sensors+=1\n",
    "                    if pred_type != true_type: \n",
    "                        n_wrong_confident_sensor_pred+=1 \n",
    "\n",
    "                    # append these sensors into labeled ones (with possibly wrong labels): \n",
    "                    sensor_bow.append(bow_array[equip_map[p]['sensor_ids'][k]]) \n",
    "                    sensor_labels.append(pred_type) \n",
    "\n",
    "                if flag==1:                 \n",
    "                    \n",
    "                    # append these sensors into labeled ones (with ground truth): \n",
    "                    sensor_bow.append(bow_array[equip_map[p]['sensor_ids'][k]]) \n",
    "                    sensor_labels.append(true_type) \n",
    "\n",
    "            break\n",
    "        #sensor_bow = np.array(sensor_bow)\n",
    "        #sensor_labels = np.array(sensor_labels)\n",
    "        #model.fit(sensor_bow, sensor_labels)\n",
    "        \n",
    "    return n_manually_labeled_thisepoch, n_wrong_confident_sensor_pred, n_high_confidence_sensors, len(sensor_labels), 100.0*len(sensor_labels)/len(bow_array),change_thresholds,len(bow_array)-len(sensor_labels)   \n",
    "    # return __  , __ , __ , num labeled sensors , % labeled sensors, change_thresholds, num_sensors_in_gray "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Iteratively train RF model and call the method to apply it on all clusters. \n",
    "# When method asks us to change thresholds, then we do so. \n",
    "# Otherwise we re-train RF and try catch more sensors. \n",
    "# We also record the number of correct sensors in each iteration, the manual effort in each iteration etc. \n",
    "\n",
    "#print equip_map.keys() \n",
    "#print labeled_equip_keys \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=400, random_state=0)\n",
    "model.fit(sensor_bow,sensor_labels) \n",
    "\n",
    "num_sensors_in_gray=100\n",
    "\n",
    "T_low = 0.1\n",
    "T_high = 0.95 \n",
    "thresholds = [ (0.1,0.95), (0.1,0.9) , (0.15,0.9), (0.15,0.85), (0.2,0.85), (0.25,0.85), (0.3,0.85), (0.35,0.85), (0.4,0.85), (0.45,0.85), (0.5,0.85), (0.55,0.85), (0.6,0.85), (0.65,0.85), (0.7,0.85), (0.75,0.85), (0.8,0.85), (0.849999999,0.85) ] \n",
    "#thresholds = [ (0.1,0.9) , (0.15,0.9), (0.15,0.85), (0.2,0.85), (0.25,0.85), (0.3,0.85), (0.35,0.85), (0.4,0.85), (0.45,0.85), (0.45,0.8),(0.5,0.8), (0.55,0.8), (0.6,0.8), (0.65,0.8), (0.7,0.8), (0.75,0.8), (0.7999999,0.8) ] \n",
    "#thresholds = [ (0.1,0.7) , (0.25,0.7), (0.3,0.7), (0.3,0.7), (0.35,0.7), (0.4,0.7), (0.4,0.65),(0.45,0.65), (0.5,0.65), (0.5,0.6), (0.55,0.6), (0.5999999,0.6)] \n",
    "\n",
    "thresh_count=0 \n",
    "\n",
    "# Start iterations: \n",
    "n_manual_lab_clusters_iter = [10 ]\n",
    "n_sensors_covered_iter = [len(sensor_labels) ] \n",
    "\n",
    "while num_sensors_in_gray>0: \n",
    "    T_low,T_high = thresholds[thresh_count] \n",
    "    \n",
    "    # Re-train model: \n",
    "    model.fit(sensor_bow,sensor_labels) \n",
    "    \n",
    "    # Use model to label clusters/sensors: \n",
    "    n_manually_labeled_thisepoch, n_wrong_confident_sensor_pred, n_high_confidence_sensors, n_sens_covered, perc_coverage,change_thresholds,num_sensors_in_gray = apply_model_on_all_clusters()        \n",
    "    print n_manually_labeled_thisepoch, n_wrong_confident_sensor_pred, n_high_confidence_sensors, n_sens_covered, perc_coverage,change_thresholds,num_sensors_in_gray  \n",
    "    n_manual_lab_clusters_iter.append(n_manual_lab_clusters_iter[-1] + n_manually_labeled_thisepoch) \n",
    "    n_sensors_covered_iter.append(n_sens_covered) \n",
    "    \n",
    "    if change_thresholds: \n",
    "        thresh_count+=1 \n",
    "        print T_low, T_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This code uses regular expressions to map descriptions (and if needed, jci_name) to ground truth \n",
    "# Goal: Get a manual effort (in mapping either of above to ground truth) to coverage \n",
    "\n",
    "desc_list=[] \n",
    "jc_names_list=[] \n",
    "sensor_info = {} \n",
    "for s in sensor_list: \n",
    "    sid = s['source_id'] \n",
    "    sensor_info[sid]={} \n",
    "    d = s['description'].lower() \n",
    "    d = ''.join([i for i in d if not i.isdigit()]) #remove digits \n",
    "    d = re.sub(r\"[^\\w' ]\", \"\",  d ) # remove special chars \n",
    "    d = ' '.join(d.split()) #remove extra spaces \n",
    "    sensor_info[sid]['desc'] = d \n",
    "    desc_list.append(d)     \n",
    "    \n",
    "    j = s['jci_name'].split('.')[-1] \n",
    "    sensor_info[sid]['jci'] = j \n",
    "    jc_names_list.append(j)  \n",
    "    sensor_info[sid]['figuredout'] = False \n",
    "    \n",
    "manualeffort=[0] \n",
    "coveredsensors=[0] \n",
    "desc_map = {}\n",
    "jci_map = {} \n",
    "\n",
    "for s in sensor_list: \n",
    "    sid = s['source_id'] \n",
    "    if sensor_info[sid]['figuredout']==True: continue # If label known, skip. \n",
    "        \n",
    "    # Info about this sensor: \n",
    "    gt = sensor_type_map[ s['source_id'] ]  # ground truth \n",
    "    d = s['description'].lower() \n",
    "    d = ''.join([i for i in d if not i.isdigit()]) #remove digits \n",
    "    d = re.sub(r\"[^\\w' ]\", \"\",  d ) # remove special chars \n",
    "    d = ' '.join(d.split()) #remove extra spaces \n",
    "    j = s['jci_name'].split('.')[-1] \n",
    "\n",
    "    \n",
    "    if not d ==\"\": \n",
    "        if not d in desc_map: \n",
    "            manualeffort.append(manualeffort[-1]+1) \n",
    "            desc_map[d] = gt \n",
    "            jci_map[j] = gt \n",
    "            sensor_info[sid]['figuredout']=True \n",
    "            # Check how many it catches: \n",
    "            numcatches=0\n",
    "            for s2 in sensor_list: \n",
    "                sid2 = s2['source_id'] \n",
    "                if sensor_info[sid2]['figuredout']==False and (sensor_info[sid2]['desc']==d   or sensor_info[sid2]['jci']==j): \n",
    "                    sensor_info[sid2]['figuredout']=True\n",
    "                    numcatches+=1 \n",
    "            coveredsensors.append(coveredsensors[-1] + numcatches) \n",
    "            \n",
    "    else: \n",
    "        if not j in jci_map: \n",
    "            manualeffort.append(manualeffort[-1]+1) \n",
    "            jci_map[j] = gt \n",
    "            sensor_info[sid]['figuredout']=True \n",
    "            # Check how many it catches: \n",
    "            numcatches=0\n",
    "            for s2 in sensor_list: \n",
    "                sid2 = s2['source_id'] \n",
    "                if sensor_info[sid2]['figuredout']==False and sensor_info[sid2]['jci']==j: \n",
    "                    sensor_info[sid2]['figuredout']=True\n",
    "                    numcatches+=1 \n",
    "            coveredsensors.append(coveredsensors[-1] + numcatches) \n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "# Just checking. \n",
    "for s in sensor_list: \n",
    "    sid = s['source_id'] \n",
    "    if sensor_info[sid]['figuredout']==False: \n",
    "        print sid \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "# Plot the manual effort vs coverage for Regex based approach. \n",
    "plt.plot(manualeffort, coveredsensors, 'ro')\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.ylabel('# points covered', fontsize=20)\n",
    "plt.xlabel('Manual inputs', fontsize=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.xticks(np.arange(0, max(manualeffort)+1, 75.))\n",
    "\n",
    "plt.savefig(\"BonnersensorsREGEXManualVsCoverage.pdf\",bbox_inches='tight',dpi=150)\n",
    "\n",
    "\n",
    "len(set(jc_names_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
